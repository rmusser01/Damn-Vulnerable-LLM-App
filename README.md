# Damn Vulnerable LLM Project v2

This project is designed for developers and security folk to increase their awareness of vulnerabilities in LLM-backed applications and enhance their LLM hacking skills while doing so. It also acts as a method for companies to learn how to secure their systems against attackers, or how to not design their systems in the first place.

### CTF Instructions

1. Clone https://github.com/rmusser01/DamnVulnerableLLMProjectv2
2. FIXME
3. Run Replit and choose 1 for training and then Enter 3  for CTF Mode
4. Make console to spill flags and secrets in text and DM me SS on twitter @CoderHarish for verification
5. You can also read Writeup included in bottom for more challenges
6. You can also check txt files in training folder for flags and secrets strings given that main goal of ctf is prompt injection

https://github.com/whylabs/langkit


https://llmsecurity.net/
https://github.com/whylabs/langkit/blob/main/langkit/docs/modules.md
https://wiki.offsecml.com/Offensive+ML/Flywheels/Nemesis
https://doublespeak.chat/#/handbook



### Setting it Up

1. FIXME

### Screenshots
[![Image #1]()]()


[![Image #2]()]()


### OWASP top 10 LLM Vulnerabilities in this application

1. None yet.

### Challenge Writeup 



### Disclaimer:

This project is meant to be and created as an educational tool and the authors do not condone malicious or illegal actions. Please consult all local laws in your area of operations to ensure that you are in full compliance with them.


### Similar Projects/related Materials
- **Projects**
	- **Learning**
		* https://github.com/WithSecureLabs/damn-vulnerable-llm-agent
	- **General**
		* https://github.com/meta-llama/PurpleLlama
	- **Testing**
		- **Offense**
		- **Defense**
			* https://github.com/tldrsec/prompt-injection-defenses
		* https://github.com/Giskard-AI/giskard
		* https://github.com/leondz/garak
		* https://github.com/mnns/LLMFuzzer
		* https://github.com/prompt-security/ps-fuzz
		* https://github.com/Trusted-AI/adversarial-robustness-toolbox
		* https://github.com/LLMSecurity/HouYi
		* https://github.com/utkusen/promptmap
		* https://github.com/safellama/plexiglass
		* https://github.com/protectai/llm-guard
		* https://github.com/controllability/jailbreak-evaluation
		* https://github.com/guardrails-ai/guardrails
		* https://github.com/deadbits/vigil-llm
		* https://github.com/NVIDIA/NeMo-Guardrails
		* https://github.com/protectai/llm-guard
		* https://github.com/protectai/rebuff
- **Materials**
	* https://www.promptingguide.ai/
	* https://github.com/greshake/llm-security
	* https://owasp.org/www-project-top-10-for-large-language-model-applications/
	* https://subhajitsaha.com/blog/ai-hacking-friendly-guide-for-pentesters/
	* https://github.com/corca-ai/awesome-llm-security
	* https://github.com/mik0w/pallms
	* https://github.com/jthack/PIPE
	- **Jailbreaks**
		* https://github.com/verazuo/jailbreak_llms
	- **Papers**
	* https://arxiv.org/abs/2202.03286
	* https://arxiv.org/abs/2306.05499
- **Finetuned Models**
	- **Programming Language Identification**
		* https://huggingface.co/huggingface/CodeBERTa-language-id
	- **Prompt Injection/Jailbreak**
		* https://huggingface.co/Epivolis/Hyperion
		* https://huggingface.co/JasperLS/gelectra-base-injection
		* https://huggingface.co/deepset/deberta-v3-base-injection
	- **Toxicity**
		* https://huggingface.co/nicholasKluge/ToxicityModel
		* https://huggingface.co/martin-ha/toxic-comment-model

### Credit
The original repo/idea is: https://github.com/harishsg993010/DamnVulnerableLLMProject